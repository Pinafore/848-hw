Buzz When You Know: QuizBowl Feature Engineering
=

The goal of this assignment is to understand the components of a simple QuizBowl QA system (i.e guesser, buzzer), analyze the data, and engineer a better Buzzer.

A quick review on the QuizBowl system:
-
There is a **Guesser**, which memorizes the questions and answers in the training set. For a new in-the-wild question, the guesser fetches the K closest questions in the memory and prepares the answers to those as potential responses. For this assignment, we use a [TF-IDF](https://www.wikiwand.com/en/Tf%E2%80%93idf) based Guesser provided in `tfidf_guesser.py`

Along with the guesser, we have a **Buzzer**, which buzzes (gives off a binary signal) when the system is confident to respond with its guess. For this assignment, we will be using a simple Logistic Regression based Buzzer as provided in `lr_buzzer.py`.

In practice, after each new word is consumed, the QB system tries to make new guesses using tfidf guesser over the already consumed prefix, and the Buzzer will go off if the confidence crosses the threshold, in this case 0.5

What's the task?
-

You will build on the *tf-idf guesser* by extracting useful information from its guesses and generate better features for input into the *logistic regression* classifier to do a better job of selecting whether a guess to a question is correct.

NOTE: Because the goal of this assignment is feature engineering, not classification algorithms, you may not change the underlying algorithm. You are only required to make changes in the `feateng` directory. Particularly, you must reimplement `prepare_train_inputs` and `prepare_eval_input` to achieve stronger results than baseline. You can change the outputs of tf-idf (e.g., to create a new feature) in `write_guess_json` function in `feateng/feat_utils.py`, but you may not swap out tf-idf for BM25.  Likewise, you cannot add hidden layers to the your logistic regression buzzer.

This assignment is structured in a way that approximates how classification works in the real world: features are typically underspecified (or not specified at all). You, the data digger, have to articulate the features you need. You then compete against others to provide useful predictions.

It may seem straightforward, but do not start this at the last minute. There are often many things that go wrong in testing out features, and you'll want to make sure your features work well once you've found them.

What Can You Do?
-

You can:
* Add features in `write_guess_json` function in `feateng/feat_utils.py`
* Change feature representations in `prepare_train_inputs` and `prepare_eval_input` functions.
* Exclude training data 
* Add training data

What Can't You Do?
-
Remove tf-idf as guesser or logistic regression as buzzer.
Change the logic of tfidf_guesser.py, lr_buzzer.py, qbdata.py


How to start
-
1. Play around and get used to using tfidf_guesser.py and lr_buzzer.py
2. Add a simple feature to the training data generated by tfidf_guesser.py 
3. See if it increases the accuracy on held-out data when you run logistic regression (lr_buzzer.py) or on the leaderboard
4. Rinse and repeat!


Accuracy (25+ points)
------------------------------

15 points of your score will be generated from your performance on the
the classification competition on the leaderboard.  The performance will be
evaluated on accuracy on a held-out test set.

You should be able to significantly
improve on the baseline system.  If you can
do much better than your peers, you can earn extra credit (up to 10 points).

Analysis (25 Points)
--------------

The job of the written portion of the homework is to convince the grader that:
* Your new features work
* You understand what the new features are doing
* You had a clear methodology for incorporating the new features

Make sure that you have examples and quantitative evidence that your
features are working well, and include the metrics you chose to measure your system's performance. Be sure to explain how used the data
(e.g., did you have a development set?) and how you inspected the
results.

A sure way of getting a low grade is simply listing what you tried and
reporting the corresponding metrics for each attempt.  You are expected to pay more
attention to what is going on with the data and take a data-driven
approach to feature engineering.

How to Turn in Your System
-
* You are only required to make code changes within `feateng` directory and provide trained TfidfGuesser model (as `tfidf.pickle`) and LogRegBuzzer model as `lr_buzzer.pickle` within `models` directory. The file provided should take care of saving the trained model at it's required place.
* You would not need to change any other python files, but feel free to play around with the code and make changes if needed. However, all the code changes outside of `feateng` would be discarded when submitted on gradescope and the Autograder would use the original version of the files provided in the repo.
**If you do not correctly save your trained model, or do not submit one at all, the autograder will fail.**
* **Custom Training Data** (If you used additional training data beyond ``small.guesstrain.json`` or ``small.buzztrain.json``
    * ``custom.guesstrain.json``: If you used additional training data for the guesser, please include the source data in your Gradescope submission in a file named ``custom.guesstrain.json`` within `custom_data` dir (if your file is <100MB).
    * ``custom.buzztrain.json``: If you used additional training data for the buzzer, please include the source data in your Gradescope submission in a file named ``custom.buzztrain.json`` within `custom_data` dir (if your file is <100MB).
    * (OR) If either of your files are >100MB, please submit a shell script named ``gather_resources.sh`` that will retrieve one or both of the files above programatically from a public location (i.e a public S3 bucket).
* ``analysis.pdf``: Your **PDF** file containing your feature engineering analysis.

Turn in the above files via Gradescope through a zip format such that `feateng`, `models` and `custom_data` and `requirements.txt` are all at the root level. We'll be using the leaderboard as before over Accuracy and Expected Win Probability measure.
